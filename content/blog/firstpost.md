---
title: JavaScript runtime from scratch
description: Part 1 - Getting started
date: 2025-04-06
---
I've spent almost a decade doing software engineering, and throughout that time, Node.js
has been a constant presence — sometimes powering the services I build, other times used
in the dev tools like webpack or gulp or myriad others.

When you think about it, Node.js is one strange beast. JavaScript coming from the browser
to the server is a story unlike any other language and love it or hate it, its influence
on the industry is undeniable.

But even then, with its prevalence and with how much time I've spent working with it,
I find myself really missing the complete picture of its internals. Of course, I have
heard the terms "V8" and "libuv" thrown around, and naturally I know what an event loop
is. But somehow, I feel like there's still a lot of handwaving in my own conceptual model.

Recently I had a cathartic experience of reading a book on Linux kernel development and
reading the kernel code with the book as a guide. Suddenly the magic and handwaving
disappears and a lot of things click and start making sense. For example, the difference
between docker and a VM becomes obvious when it was quite murky before (of course it's
just cgroups + namespaces, duh!).

In this blog series, I'd like to recreate this feeling. To understand exactly how it works
we're going to build a JavaScript runtime from scratch, starting from compiling and linking
V8 and going from there. We'll look at Node's source code to see how it ticks inside and
consult the ECMAScript spec for answers. By the end of this series, we will understand
exactly how that strange beast works and how it all comes together and hopefully have
a fully working http server running in JS that we've built from scratch.

## Revving up the engine

To get started, we'd like to be able to run JavaScript. It just so happens that a few
companies have been obsessing over running JavaScript as fast as humanly possible as if
their life depended on it. The top choices are JavaScript Core from Apple and V8 from
Google, and we'll deny ourselves the pleasure of contrarianism and stick to V8.

Most of the steps to get started with V8 are described in the
[embedding v8 guide](https://v8.dev/docs/embed). You will need git on your system. With it,
you need to clone Google's cli tool `depot_tools` and add it to the system path. Once
added, we can use the tools to fetch the source.

```bash
mkdir ~/v8
cd ~/v8
fetch v8
cd v8
```

The whole repository weighs 5.6 GB after fetch is done, so it might take a while on a
slower connection.

Once fetched, the repo is in detached HEAD state. We'd like to build a stable
version of the engine, so we will have to check it out. The released versions in v8 repo
are saved on `branch-heads/${TAG_VERSION}` tags. I initially attempted to use the same
version that current version of Node used (23.7.0 for Node and 12.9 version of V8). But
compiled binaries crashed with segfaults during dynamic libraries initialization. So I
resolved to use a version mentioned in the embedding guide. That was 13.1. To
check it out:

```bash
git checkout branch-heads/13.1
```

Once checked out, sync all the Git modules:
```bash
gclient sync
```

Before we start compiling, we need to create a build configuration:
```bash
tools/dev/v8gen.py x64.release.sample
```

This creates a build configuration inside `v8/out.gn/args.gn` file.
Once the build config is generated, we can start the compilation:

```bash
ninja -C out.gn/x64.release.sample v8_monolith
```

What we are building here is a `v8_monolith` target that is specifically used for
embedding as it creates a single static library, compared to a swath of multiple shared
libraries generated by the default configuration. `sample` configuration is just one of
the default configs in v8 that are available as starting points to be customized later.
`release` as opposed to `debug` generates a build without debug symbols, which we won't
have use for yet because we're not adept enough at v8 internals. And `x64` is of course
the architecture.
#### A side note on tools used
Ninja — a minimalistic build system
GN — GenerateNinja, a generator for Ninja configs

The build process is memory heavy. I gave my VM 10 gigs of RAM and 10 gigs of swap, and it
took 50 minutes to compile and still ran out of memory close to the end and ultimately
failed. Thankfully, Ninja can pick up from a failed job and once restarted, it finished
the rest of the tasks. There's a way to reduce memory consumption of the build using
the `-j` argument of Ninja and limit the number of jobs running in parallel, but I didn't
end up using it:

```bash
ninja -C out.gn/x64.release.sample v8_monolith -j 4
```

One more thing we need to do to be able to run our own code is to copy `icudtl.dat` to the
location of our binary. ICU stands for International Components for Unicode, and it
provides character encoding, locale-specific behavior, and other internationalization
features that V8 relies on. We can copy it from the v8 build folder:

```bash
cp out.gn/x64.release.sample/icudtl.dat .
```

### Taking it for a spin

Okay, we now have built the mighty v8. Let's now whip up a small C++ app that will allow
us to run JavaScript files with it.

Sidenote on d8: V8's build includes a REPL called d8, which allows you to run JS in V8
directly, but we have not built it because we'd like to embed the engine. It's possible to
build it using this command:
```bash
ninja -C out.gn/x64.release.sample d8
```

For the start, we will just take Google's own [hello world example](https://chromium.googlesource.com/v8/v8/+/branch-heads/11.9/samples/hello-world.cc) with only a few
modifications for reading a script from the filesystem. The whole source can be viewed
[on github](https://github.com/main-kun/notjs/blob/main/simple-runner/simple-runner.cc), but here we will discuss the meaty part:

```cpp
  v8::Isolate::CreateParams create_params;
  create_params.array_buffer_allocator =
      v8::ArrayBuffer::Allocator::NewDefaultAllocator();
  v8::Isolate* isolate = v8::Isolate::New(create_params);
  {
    v8::Isolate::Scope isolate_scope(isolate);

    v8::HandleScope handle_scope(isolate);

    v8::Local<v8::Context> context = v8::Context::New(isolate);

    v8::Context::Scope context_scope(context);
    {
      const std::optional<std::string> js_code =
          ReadFile(argv[1]);
      if (!js_code) {
        return 1;
      }

      v8::Local<v8::String> source =
          v8::String::NewFromUtf8(
            isolate,
            js_code.value().c_str(),
            v8::NewStringType::kNormal)
                .ToLocalChecked();

      v8::Local<v8::Script> script =
          v8::Script::Compile(context, source)
            .ToLocalChecked();

      v8::Local<v8::Value> result = script->Run(context)
           .ToLocalChecked();

      v8::String::Utf8Value utf8(isolate, result);
      printf("%s\n", *utf8);
    }
  }

```

We can see here that first an Isolate is created, after that a Context using the Isolate.
The context is then used to compile and execute the script.

So what are the Isolate and the Context? Well, the Isolate, as the name suggests, is an
isolated instance of the JavaScript VM. It is the meat of JS execution in V8. It contains
its own JS heap, garbage collector, compiler instance and other core runtime components.
Multiple Isolates can run in the same process without interfering with each other.

What's a Context? Context closely corresponds to what the ECMAScript specification
calls a [Realm](https://tc39.es/ecma262/#sec-code-realms). It provides the root object of JS execution and necessary built-in
objects, like Object, Array, Promise, etc. When JS code is executed, it always runs within
a Context, which determines the functions and global variables available to the code. A
single Isolate may contain multiple Contexts.

Does Node.js do the same as our little hello-world? Well, yes and no. If you follow Node's
own node.cc in src folder, you will find `NodeMainInstance` class used inside
`StartInternal` function:

```cpp
// node/src/node.cc
static ExitCode StartInternal(int argc, char** argv) {
  // Other setup stuff
  // ....
  NodeMainInstance main_instance(snapshot_data,
                                 uv_default_loop(),
                                 per_process::v8_platform.Platform(),
                                 result->args(),
                                 result->exec_args());
  return main_instance.Run();
}
```

Inside `NodeMainInstance` constructor the same `Isolate` we're using here is initialized
in a similar fashion:

```cpp
// node/src/node_main_instance.cc
NodeMainInstance::NodeMainInstance(const SnapshotData* snapshot_data,
                                   uv_loop_t* event_loop,
                                   MultiIsolatePlatform* platform,
                                   const std::vector<std::string>& args,
                                   const std::vector<std::string>& exec_args)
    : args_(args),
      exec_args_(exec_args),
      array_buffer_allocator_(ArrayBufferAllocator::Create()),
      isolate_(nullptr),
      platform_(platform),
      isolate_data_(),
      isolate_params_(std::make_unique<Isolate::CreateParams>()),
      snapshot_data_(snapshot_data) {
  isolate_params_->array_buffer_allocator = array_buffer_allocator_.get();

  // Isolate created here.
  isolate_ =
      NewIsolate(isolate_params_.get(), event_loop, platform, snapshot_data);
// etc
```

But Context initialization is a bit trickier. To save time, Node uses a pre-generated
snapshot of v8 context to initialize the environment. We can see it in
`CreateMainEnvironment` function, after the `snapshot_data` check:

```cpp
// node/src/node_main_instance.cc
NodeMainInstance::CreateMainEnvironment(ExitCode* exit_code) {
  *exit_code = ExitCode::kNoFailure;

  HandleScope handle_scope(isolate_);

  if (isolate_data_->options()->track_heap_objects) {
    isolate_->GetHeapProfiler()->StartTrackingHeapObjects(true);
  }

  Local<Context> context;
  DeleteFnPtr<Environment, FreeEnvironment> env;

  if (snapshot_data_ != nullptr) {
  // Create environment from snapshot
    env.reset(CreateEnvironment(isolate_data_.get(),
                                Local<Context>(),  // read from snapshot
                                args_,
                                exec_args_));
#if HAVE_OPENSSL
    crypto::InitCryptoOnce(isolate_);
#endif  // HAVE_OPENSSL
  } else {
    context = NewContext(isolate_);
    CHECK(!context.IsEmpty());
    Context::Scope context_scope(context);
    env.reset(
        CreateEnvironment(isolate_data_.get(), context, args_, exec_args_));
  }

  return env;
}

```

This makes sense since at the start of execution, the environment is always the same. If
the snapshot is not available, Node initializes a new `Context` with an `Isolate` just
like we did.

### Have we done Node.js now?

We've got ourselves a binary that takes in a JavaScript file and runs it. Have we actually
created our own Node.js alternative? Will VCs barge into my door with bags full of money
begging to fund this new revolutionary technology? I don't know, let's see what works.

So, simple things work as expected. You can define functions, use operators and use
built-ins.

```js
function test_builtin (a, b) {
    return Math.floor(a/b);
}
test_builtin(5, 2);

// returns 2
```

The built-ins are rather limited. Printing the contents of `globalThis` using this script.

```js
JSON.stringify(Object.getOwnPropertyNames(globalThis));
```

returns the following list
```js
["Object","Function","Array","Number","parseFloat","parseInt","Infinity","NaN",
    "undefined","Boolean","String","Symbol","Date","Promise","RegExp","Error",
    "AggregateError","EvalError","RangeError","ReferenceError","SyntaxError",
    "TypeError","URIError","globalThis","JSON","Math","Intl","ArrayBuffer","Atomics",
    "Uint8Array","Int8Array","Uint16Array","Int16Array","Uint32Array","Int32Array",
    "Float32Array","Float64Array","Uint8ClampedArray","BigUint64Array","BigInt64Array",
    "DataView","Map","BigInt","Set","WeakMap","WeakSet","Proxy","Reflect",
    "FinalizationRegistry", "WeakRef","decodeURI","decodeURIComponent","encodeURI",
    "encodeURIComponent","escape","unescape","eval","isFinite","isNaN","console",
    "Iterator","SharedArrayBuffer","WebAssembly"]
```

Contrary to my expectations, `console` is actually here! However, `console.log` inside
the script does not seem to be doing anything currently. The list of built-ins is much
shorter than Node's. Running the same script in Node, we get a list about two times longer.

```js
["Object","Function","Array","Number","parseFloat","parseInt","Infinity","NaN","undefined"
    ,"Boolean","String","Symbol","Date","Promise","RegExp","Error","AggregateError",
    "EvalError","RangeError","ReferenceError","SyntaxError","TypeError","URIError",
    "globalThis","JSON","Math","Intl","ArrayBuffer","Uint8Array","Int8Array",
    "Uint16Array","Int16Array","Uint32Array","Int32Array","Float32Array","Float64Array",
    "Uint8ClampedArray","BigUint64Array","BigInt64Array","DataView","Map","BigInt","Set",
    "WeakMap","WeakSet","Proxy","Reflect","FinalizationRegistry","WeakRef","decodeURI",
    "decodeURIComponent","encodeURI","encodeURIComponent","escape","unescape","eval",
    "isFinite","isNaN","console","SharedArrayBuffer","Atomics","WebAssembly","process",
    "global","Buffer","queueMicrotask","clearImmediate","setImmediate","structuredClone",
    "URL","URLSearchParams","DOMException","clearInterval","clearTimeout","setInterval",
    "setTimeout","BroadcastChannel","AbortController","AbortSignal","Event","EventTarget",
    "MessageChannel","MessagePort","MessageEvent","atob","btoa","Blob","Performance",
    "performance","TextEncoder","TextDecoder","TransformStream",
    "TransformStreamDefaultController","WritableStream","WritableStreamDefaultController",
    "WritableStreamDefaultWriter","ReadableStream","ReadableStreamDefaultReader",
    "ReadableStreamBYOBReader","ReadableStreamBYOBRequest","ReadableByteStreamController",
    "ReadableStreamDefaultController","ByteLengthQueuingStrategy","CountQueuingStrategy",
    "TextEncoderStream","TextDecoderStream","CompressionStream","DecompressionStream",
    "fetch","FormData","Headers","Request","Response"]
```

But it becomes obvious this way where exactly Node's own global definitions start. Right
after "WebAssembly", starting from "process". Even the order, starting from "process",
"global" and "Buffer" makes sense in terms of its importance for Node.

### Are we async yet?

The state of async at this point is rather interesting. As we've seen previously,
`Promise` built-in is defined within our environment.  However, we don't have any ways to
interact with the network yet, so no `fetch` and no `http`. Even simple async scheduling
with `setTimeout` is not available for us. So what can we do at this point?

Well, resolve promises, of course. The reason for our current state of affairs is that
ECMAScript spec defines [Jobs](https://tc39.es/ecma262/#job), which translates to micro-task queue that V8
implements. That is V8's responsibility, as opposed to network and timers, which are
macro tasks and are implemented by the environment.

So let's whip up a script to validate that promises do in fact get resolved:

```js
let message = "Initial value";

Promise.resolve().then(() => {
    message = "Promise was processed!";
});

message;
```

But running this script in our current implementation results in the `Initial value` being
printed to terminal. Why? Because the micro task queue has not been processed. To actually
process the task queue, let's do a quick and dirty modification to our program. We will
force V8 to process the queue and output it to our C++ code again:

```c++

// PerformMicrotaskCheckpoint runs the queue until it's empty
isolate->PerformMicrotaskCheckpoint();

v8::Local<v8::String> checkSource =
        v8::String::NewFromUtf8(isolate, "message;", v8::NewStringType::kNormal).ToLocalChecked();
v8::Local<v8::Script> checkScript =
        v8::Script::Compile(context, checkSource).ToLocalChecked();
result = checkScript->Run(context).ToLocalChecked();

v8::String::Utf8Value utf8(isolate, result);
printf("%s\n", *utf8);

```

The full example with processing the queue is available on [GitHub](https://github.com/main-kun/notjs/blob/main/simple-runner/simple-runner-microtask.cc)
Now running the compiled binary we get `"Promise was processed!"` as expected.

### The Road Goes Ever On

Let's take stock of what we figured out so far. We learned how to build V8 and embed it,
we checked out how Node.js does the things we are doing here (and found an interesting
optimization), we've experimented a bit with running scripts and saw with our own eyes
the difference between micro and macro tasks. But even though we can now run a JS script
ourselves using embedded V8 engine, it's clear that we are pretty far from being able to
serve http requests with this thing. Next time, we will take a step back from JavaScript
specifics and discuss what options we have in handling requests on a web server and how
a certain loop of events might help us out (or maybe complicate things further).